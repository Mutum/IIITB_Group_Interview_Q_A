
Q.1 MORGAN STANLEY: did you use Dimensionality reduction technique , what , how  [ PCA , was already discussed]
A. cor matrix, 

Q.2 MORGAN STANLEY: Advantage of Lasso Over Ridge [ so i figured out aftyer some google Lasso can double as Dim redux ]

Q.3 MORGAN STANLEY: In your project, Did you create additional feature, if so how and why

Q.4 MORGAN STANLEY: SVM regularization .. fully grilled here, with ref to R implementation 

Q.5 MORGAN STANLEY: logistic regression .. explain all formulas and meanings 

1] odds ration
2] log of odds ration
3] loss function etc 


Q.6 MORGAN STANLEY:in NN, how to avoid vanishing gradient 

Q.6.2 MORGAN STANLEY: in RNN, how to avoid exploding gradient 
use LTSM memory gates 

Q.7 How did i decided K in my K-means clustering [ scatter plot etc ]

Q.8 BOSCH:  Liner regression formula for y = mx +c 

1] 
y = mx + c
m = (n*sum(XiYi) - sum(Xi)*sum(Yi)) /  (n * sum(Xi^2) -  sum(Xi) ^2 )
c =  ( sum(Yi) * sum(Xi^2) - sum(Xi)* sum(Xi * Yi) ) / (n * sum(Xi^2) -  sum(Xi) ^2 )

2] β=(X'X)^−1 X'Y

3] Gradient Descent 
use GD over analytical solution if:

you are considering changes in the model, generalizations, adding some more complex terms/regularization/modifications
you need a generic method because you do not know much about the future of the code and the model (you are only one of the developers)
analytical solution is more expensive computationaly, and you need efficiency
analytical solution requires more memory, which you do not have
analytical solution is hard to implement and you need easy, simple code

Risk of meeting different local optimum
The linear regression cost function is always a convex function - always has a single minimum
Bowl shaped
One global optima
So gradient descent will always converge to global optima


Q.9 How do we compare between diffrent models ? which is best model for a given probem ?

Q.10 Whats the problem if we set all weights to ZERO in NN ?
see this answer 
https://stats.stackexchange.com/questions/27112/danger-of-setting-all-initial-weights-to-zero-in-backpropagation

Im pretty sure that backpropagation involves adding to the existing weights, not multiplying. The amount that you add is specified by the delta rule. Note that wij doesn't appear on the right-hand-side of the equation.

My understanding is that there are at least two good reasons not to set the initial weights to zero:

First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values. You can't do that if they all start at zero.
Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another.


Q.11 What is the problem with sigmoid activation function?
A. see comparision:   [some one needs to make it more concise ]
 Sigmoid. The sigmoid non-linearity has the mathematical form σ(x)=1/(1+e−x)σ(x)=1/(1+e−x) 
 it takes a real-valued number and “squashes” it into range between 0 and 1. 
 In particular, large negative numbers become 0 and large positive numbers become 1. 

Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero.
Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.
Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x>0x>0 elementwise in f=wTx+bf=wTx+b)), then the gradient on the weights ww will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression ff). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.

Tanh. The tanh non-linearity .
It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate,but unlike the sigmoid neuron its output is zero-centered.
Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: tanh(x)=2σ(2x)−1tanh⁡(x)=2σ(2x)−1.

  
Left: Rectified Linear Unit (ReLU) activation function, which is zero when x < 0 and then linear with slope 1 when x > 0. Right: A plot from Krizhevsky et al. (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.
ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function f(x)=max(0,x)f(x)=max(0,x). In other words, the activation is simply thresholded at zero (see image above on the left). There are several pros and cons to using the ReLUs:

(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.
(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.
(-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.
Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes f(x)=1(x<0)(αx)+1(x>=0)(x)f(x)=1(x<0)(αx)+1(x>=0)(x) where αα is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.

Maxout. Other types of units have been proposed that do not have the functional form f(wTx+b)f(wTx+b) where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function max(wT1x+b1,wT2x+b2)max(w1Tx+b1,w2Tx+b2). Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1=0w1,b1=0). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.

This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.

Q11] WIPRO : In R , there is a DF with 13 cols such as JAN, FEB ., MAR, APR .. DEC & CUSTID
Now each col contains Transaction Number 
Ans: 
for each cust id find highest Month 
custid <-  c(1,2,3,4,5,6,7,8,9,10,11,12 )
jan <-  c(1,2,3,4,5,6,7,8,9,10,11,120 )
feb  <-  c(2,3,4,5,6,7,8,9,10,11,120,1 )
mar <-  c(3,4,5,6,7,8,9,10,11,120,1,2 )
apr <-  c(4,5,6,7,8,9,10,11,120,1,2,3 )
may <-  c(100,2,3,4,5,6,7,8,9,10,11,12 )
jun <-  c(1,200,3,4,5,6,7,8,9,10,11,12 )
jul <-  c(1,2,300,4,5,6,7,8,9,10,11,12 )
aug  <-  c(1,2,3,400,5,6,7,8,9,10,11,12 )
sep <-  c(1,2,3,4,500,6,7,8,9,10,11,12 )
oct <-  c(1,2,3,4,5,600,7,8,9,10,11,12 )
nov<-  c(1,2,3,4,5,6,700,8,9,10,11,12 )
dec <-  c(1,2,3,4,5,6,7,800,9,10,11,12 )
trdf <- data.frame( custid=custid , jan, feb , mar, apr, may , jun, jul, aug, sep, oct, nov, dec)
colnames(trdf)[max.col(trdf,ties.method="first")]

Q12] WIPRO :  Losgictic regresion , 1000 independent variables , which variable is most important in logistivc regression 

Q13] WIPRO : what are RF tuning parameters , 
how do u select which is best parameter 

